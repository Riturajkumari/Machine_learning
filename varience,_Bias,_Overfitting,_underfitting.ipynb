{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riturajkumari/Machine_learning/blob/main/varience%2C_Bias%2C_Overfitting%2C_underfitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690af39f",
      "metadata": {
        "id": "690af39f"
      },
      "source": [
        "**Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
        "can they be mitigated?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eab7172",
      "metadata": {
        "id": "5eab7172"
      },
      "source": [
        "**Overfitting** \n",
        "- Overfitting occurs when our machine learning model tries to cover all the data points or more than the required data points present in the given dataset. Because of this, the model starts caching noise and inaccurate values present in the dataset, and all these factors reduce the efficiency and accuracy of the model. The overfitted model has low bias and high variance.\n",
        "- overfitted when the model does not make accurate predictions on testing data. When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing with test data results in High variance. \n",
        "- The causes of overfitting are the non-parametric and non-linear methods because these types of machine learning algorithms have more freedom in building the model based on the dataset and therefore they can really build unrealistic models. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f633ae0",
      "metadata": {
        "id": "9f633ae0"
      },
      "source": [
        "Overfitting is a problem where the evaluation of machine learning algorithms on training data is different from unseen data.\n",
        "\n",
        "**Reasons for Overfitting are as follows:**\n",
        "- High variance and low bias \n",
        "- The model is too complex\n",
        "- The size of the training data "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96fdbcb9",
      "metadata": {
        "id": "96fdbcb9"
      },
      "source": [
        "**underfitting**\n",
        "- Underfitting destroys the accuracy of our machine learning model. Its occurrence simply means that our model or the algorithm does not fit the data well enough. It usually happens when we have fewer data to build an accurate model and also when we try to build a linear model with fewer non-linear data.\n",
        "- rules of the machine learning model are too easy and flexible to be applied to such minimal data and therefore the model will probably make a lot of wrong predictions. Underfitting can be avoided by using more data and also reducing the features by feature selection. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e633bd07",
      "metadata": {
        "id": "e633bd07"
      },
      "source": [
        "Underfitting refers to a model that can neither performs well on the training data nor generalize to new data.\n",
        "\n",
        "**Reasons for Underfitting:**\n",
        "\n",
        "- High bias and low variance \n",
        "- The size of the training dataset used is not enough.\n",
        "- The model is too simple.\n",
        "- Training data is not cleaned and also contains noise in it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "234619b9",
      "metadata": {
        "id": "234619b9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f73e6ded",
      "metadata": {
        "id": "f73e6ded"
      },
      "source": [
        "**Q2: How can we reduce overfitting? Explain in brief.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd67da99",
      "metadata": {
        "id": "dd67da99"
      },
      "source": [
        "**Techniques to reduce overfitting:**\n",
        "\n",
        "- Increase training data.\n",
        "- Reduce model complexity.\n",
        "- Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\n",
        "- Ridge Regularization and Lasso Regularization\n",
        "- Use dropout for neural networks to tackle overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b70169f9",
      "metadata": {
        "id": "b70169f9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "1f148244",
      "metadata": {
        "id": "1f148244"
      },
      "source": [
        "**Q3: Explain underfitting. List scenarios where underfitting can occur in ML.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebaed56d",
      "metadata": {
        "id": "ebaed56d"
      },
      "source": [
        "**Underfitting**\n",
        "-  it only performs well on training data but performs poorly on testing data. \n",
        "- Underfitting destroys the accuracy of our machine learning model.\n",
        "- Its occurrence simply means that our model or the algorithm does not fit the data well enough. \n",
        "- It usually happens when we have fewer data to build an accurate model and also when we try to build a linear model with fewer non-linear data. \n",
        "- the rules of the machine learning model are too easy and flexible to be applied to such minimal data and therefore the model will probably make a lot of wrong predictions. \n",
        "- Underfitting can be avoided by using more data and also reducing the features by feature selection. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee51fa93",
      "metadata": {
        "id": "ee51fa93"
      },
      "source": [
        "**Techniques to reduce underfitting:**\n",
        "\n",
        "- Increase model complexity\n",
        "- Increase the number of features, performing feature engineering\n",
        "- Remove noise from the data.\n",
        "- Increase the number of epochs or increase the duration of training to get better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0445f0f0",
      "metadata": {
        "id": "0445f0f0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "58e2eacd",
      "metadata": {
        "id": "58e2eacd"
      },
      "source": [
        "**Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
        "variance, and how do they affect model performance?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a99a6040",
      "metadata": {
        "id": "a99a6040"
      },
      "source": [
        "- The **bias** is known as the difference between the prediction of the values by the ML model and the correct value. Being high in biasing gives a large error in training as well as testing data. Its recommended that an algorithm should always be low biased to avoid the problem of underfitting.\n",
        "- By high bias, the data predicted is in a straight line format, thus not fitting accurately in the data in the data set. Such fitting is known as Underfitting of Data. This happens when the hypothesis is too simple or linear in nature."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be5e7aa8",
      "metadata": {
        "id": "be5e7aa8"
      },
      "source": [
        "**Variance of the model**\n",
        "- The variability of model prediction for a given data point which tells us spread of our data is called the variance of the model. \n",
        "- The model with high variance has a very complex fit to the training data and thus is not able to fit accurately on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data.\n",
        "- When a model is high on variance, it is then said to as Overfitting of Data. Overfitting is fitting the training set accurately via complex curve and high order hypothesis but is not the solution as the error with unseen data is high."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be790e62",
      "metadata": {
        "id": "be790e62"
      },
      "source": [
        "**Bias Variance Tradeoff**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a50eb194",
      "metadata": {
        "id": "a50eb194"
      },
      "source": [
        "- If the algorithm is too simple (hypothesis with linear eq.) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex ( hypothesis with high degree eq.) then it may be on high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as Trade-off or Bias Variance Trade-off.\n",
        "- This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8c989ec",
      "metadata": {
        "id": "b8c989ec"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "259d3fd9",
      "metadata": {
        "id": "259d3fd9"
      },
      "source": [
        "**Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
        "How can you determine whether your model is overfitting or underfitting?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1c0dbf5",
      "metadata": {
        "id": "e1c0dbf5"
      },
      "source": [
        "- Overfitting refers to a model that models the training data too well.\n",
        "- Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data.\n",
        "- A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the underlying trend of the data, i.e., it only performs well on training data but performs poorly on testing data.\n",
        "- Overfitting: A statistical model is said to be overfitted when the model does not make accurate predictions on testing data.\n",
        "-  When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing with test data results in High variance. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "508b094c",
      "metadata": {
        "id": "508b094c"
      },
      "source": [
        "**Reasons for Overfitting are as follows:**\n",
        "- High variance and low bias \n",
        "- The model is too complex\n",
        "- The size of the training data "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "610da36e",
      "metadata": {
        "id": "610da36e"
      },
      "source": [
        "**Techniques to reduce overfitting:**\n",
        "\n",
        "- Increase training data.\n",
        "- Reduce model complexity.\n",
        "- Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\n",
        "- Ridge Regularization and Lasso Regularization\n",
        "- Use dropout for neural networks to tackle overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8240ef8",
      "metadata": {
        "id": "b8240ef8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "be786d43",
      "metadata": {
        "id": "be786d43"
      },
      "source": [
        "**Underfitting**\n",
        "- Underfitting destroys the accuracy of our machine learning model.\n",
        "-  machine learning model are too easy and flexible to be applied to such minimal data and therefore the model will probably make a lot of wrong predictions.\n",
        "- Underfitting can be avoided by using more data and also reducing the features by feature selection. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78d45a76",
      "metadata": {
        "id": "78d45a76"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9df50268",
      "metadata": {
        "id": "9df50268"
      },
      "source": [
        "**Reasons for Underfitting:**\n",
        "\n",
        "- High bias and low variance \n",
        "- The size of the training dataset used is not enough.\n",
        "- The model is too simple.\n",
        "- Training data is not cleaned and also contains noise in it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afc2a735",
      "metadata": {
        "id": "afc2a735"
      },
      "source": [
        "**Techniques to reduce underfitting:** \n",
        "\n",
        "- Increase model complexity\n",
        "- Increase the number of features, performing feature engineering\n",
        "- Remove noise from the data.\n",
        "- Increase the number of epochs or increase the duration of training to get better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2f8c986",
      "metadata": {
        "id": "a2f8c986"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5129130d",
      "metadata": {
        "id": "5129130d"
      },
      "source": [
        "**Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
        "and high variance models, and how do they differ in terms of their performance?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc051f5b",
      "metadata": {
        "id": "dc051f5b"
      },
      "source": [
        "**Bias**\n",
        "- Bias is a phenomenon that occurs in the machine learning model where in an algorithm is used and it does not fit properly.\n",
        "- Bias refers to the difference between predicted values and actual values.\n",
        "- The model cannot find patterns in the training dataset and fails for both seen and unseen data.\n",
        "- Bias is a phenomenon that occurs in the machine learning model wherein you have used an algorithm and it does not fit properly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5587ef26",
      "metadata": {
        "id": "5587ef26"
      },
      "source": [
        "**Variance**\n",
        "- Variance is the change in prediction accuracy of machine learning between training data and test data.\n",
        "\n",
        "- Variance specifies the amount of variation that the estimate of the target function will change if different training data was used.\n",
        "- The model finds most patterns in the dataset and even learns from the unnecessary data or the noise.\n",
        "- Variance depends on a single training set and it determines the inconsistency of different predictions using different training sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b020bf49",
      "metadata": {
        "id": "b020bf49"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "cc0d42d2",
      "metadata": {
        "id": "cc0d42d2"
      },
      "source": [
        "**Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
        "some common regularization techniques and how they work.?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59d4f2c7",
      "metadata": {
        "id": "59d4f2c7"
      },
      "source": [
        "**regularization in machine learning**\n",
        "- Regularization is a technique used to reduce the errors by fitting the function appropriately on the given training set and avoid overfitting. \n",
        "The commonly used regularization techniques are : \n",
        "  - L1 regularization\n",
        "  - L2 regularization\n",
        "  - Dropout regularization\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e475da8",
      "metadata": {
        "id": "6e475da8"
      },
      "source": [
        "focus on L1 and L2 regularization. \n",
        "- A regression model which uses L1 Regularization technique is called LASSO(Least Absolute Shrinkage and Selection Operator) regression. \n",
        "- A regression model that uses L2 regularization technique is called Ridge regression. \n",
        "- Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function(L). \n",
        "- Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function(L). "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ee9c6aa",
      "metadata": {
        "id": "3ee9c6aa"
      },
      "source": [
        "NOTE that during Regularization the output function(y_hat) does not change. The change is only in the loss function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6ad6c47",
      "metadata": {
        "id": "f6ad6c47"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}